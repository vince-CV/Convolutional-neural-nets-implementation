{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Case Study: AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features: ReLU, Dropout, LRN, GPU..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet reinforced the idea of the CNN structure LeCun proposed:<br>\n",
    "* Sigmoid suffered from gradient dispersion, ReLU implemented instead.\n",
    "* Dropout to ignore some of the neurons in the last FC layers.\n",
    "* Overlapping max-pooling rather than the average-pooling.\n",
    "* Utilize LRN to mimic side inhibition, which increased the generalization of the model.\n",
    "* CUDA acceleration, 2*GTX 580 GPU.\n",
    "* Data Augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet structure as shown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](AlexNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of AlexNet. It is a large time consumer to train the AlexNet on ImageNet dataset. Here, I am not going into the ImageNet data, but the complete structure will be fulfilled, and test the training speed of the Forward-prop and Back-prop in each signal batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = 100\n",
    "\n",
    "def report_info(x):\n",
    "    print(x.op.name, ' ',x.get_shape().as_list())\n",
    "\n",
    "def inference(images):\n",
    "    params = []\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([11,11,3,64], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        conv   = tf.nn.conv2d(images, kernel, [1,4,4,1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32), trainable=True, name='biases')\n",
    "        bias   = tf.nn.bias_add(conv,biases)\n",
    "        conv1  = tf.nn.relu(bias, name=scope)\n",
    "        \n",
    "        report_info(conv1)\n",
    "        params += [kernel, biases]\n",
    "    \n",
    "    lrn1 = tf.nn.lrn(conv1, 4, bias=1, alpha=0.001/9, beta=0.75, name='lrn1')\n",
    "    pool1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='pool1')\n",
    "\n",
    "    report_info(pool1)\n",
    "\n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([5,5,64,192], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        conv   = tf.nn.conv2d(pool1, kernel, [1,1,1,1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[192], dtype=tf.float32), trainable=True, name='biases')\n",
    "        bias   = tf.nn.bias_add(conv,biases)\n",
    "        conv2  = tf.nn.relu(bias, name=scope)\n",
    "        \n",
    "        params += [kernel, biases]\n",
    "        report_info(conv2)\n",
    "    \n",
    "    lrn2 = tf.nn.lrn(conv2, 4, bias=1, alpha=0.001/9, beta=0.75, name='lrn2')\n",
    "    pool2 = tf.nn.max_pool(lrn2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='pool2')\n",
    "\n",
    "    report_info(pool2)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3,3,192,384], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        conv   = tf.nn.conv2d(pool2, kernel, [1,1,1,1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32), trainable=True, name='biases')\n",
    "        bias   = tf.nn.bias_add(conv,biases)\n",
    "        conv3  = tf.nn.relu(bias, name=scope)\n",
    "        \n",
    "        params += [kernel, biases]\n",
    "        report_info(conv3)\n",
    "        \n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3,3,384,256], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        conv   = tf.nn.conv2d(conv3, kernel, [1,1,1,1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')\n",
    "        bias   = tf.nn.bias_add(conv,biases)\n",
    "        conv4  = tf.nn.relu(bias, name=scope)\n",
    "        \n",
    "        params += [kernel, biases]\n",
    "        report_info(conv4)\n",
    "        \n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3,3,256,256], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        conv   = tf.nn.conv2d(conv4, kernel, [1,1,1,1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')\n",
    "        bias   = tf.nn.bias_add(conv,biases)\n",
    "        conv5  = tf.nn.relu(bias, name=scope)\n",
    "        \n",
    "        params += [kernel, biases]\n",
    "        report_info(conv5)\n",
    "        \n",
    "    pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID',name='pool5')\n",
    "    report_info(pool5)\n",
    "    \n",
    "    return pool5, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function was designed for evaluating the running time of AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_time(session, target, info):\n",
    "    num_steps_burn_in = 10 # warm-up\n",
    "    total_duration = 0.0\n",
    "    total_duration_squared = 0.0\n",
    "    \n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target)\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_steps_burn_in:\n",
    "            if not i % 10:\n",
    "                print('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "            \n",
    "        mean = total_duration / num_batches\n",
    "        vari = total_duration_squared / num_batches - mean*mean\n",
    "        stan = math.sqrt(vari)\n",
    "        \n",
    "        print('%s: %s arcoss %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), info_string, num_batches, mean, stan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1   [32, 56, 56, 64]\n",
      "pool1   [32, 27, 27, 64]\n",
      "conv2   [32, 27, 27, 192]\n",
      "pool2   [32, 13, 13, 192]\n",
      "conv3   [32, 13, 13, 384]\n",
      "conv4   [32, 13, 13, 256]\n",
      "conv5   [32, 13, 13, 256]\n",
      "pool5   [32, 6, 6, 256]\n"
     ]
    }
   ],
   "source": [
    "def benchmark():\n",
    "    with tf.Graph().as_default():\n",
    "        image_size = 224\n",
    "        images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,3],dtype=tf.float32,stddev=1e-1)) # not using ImageNet\n",
    "        pool5, parameters = inference(images)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        \n",
    "        run_time(sess, pool5, \"Forward\")\n",
    "        objective = tf.nn.l2_loss(pool5)\n",
    "        grad = tf.gradients(objective, parameters)\n",
    "        run_time(sess, grad, \"Forward-backward\")\n",
    "\n",
    "benchmark()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN training process needs more time, which requires many iterations. AlexNet won ILSVRC 2012, and reached 16.4% error. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to mention, the large dataset like ImageNet could serve the deep learning from trapping into overfitting. The traditional machine learning model needs smaller dataset, but deep learning requires large learning capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summary, CNN has strong abilities to extract features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
