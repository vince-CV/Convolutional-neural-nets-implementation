{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax Regression can be classified into multi-classification method. It can be considered a simple Neural Network without hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer enables the Neural Network with more functional features to fit more complicated and non-linear model. Theorically, the number of the hidden nodes are exponentially decreasing with the increase of the hidden layers. In other words, the more hidden layers, the less hidden layers required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real problems, many troubles could be encounted with the deeper layer in a network, such as: Overfitting, Parameters-tunning, or Gradient Vanishment. Recently, more and more tricks have came out to deal with those problems, such as Dropout, Adagrad, ReLU..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overfit can be common in machine learning.Dropout was proposed by Hinton. The idea is to drop a partial output nodes randomly when training the model.This method is equal to create more random samples, hence the overfit can be provented by enlarging the samples set.<br>\n",
    "Dropout can also be considered a bagging method, and it resample the feeatures by dropping the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parameters are difficult to be tuned. Neural Networks are ususlly not convex problem, so the results can be trapped around the local optimal. The parameters such as Learning Rate can be really matters when training with SGD algorithm, and the results could be totally different when different learning rate applied.<br>\n",
    "More adaptive methods came out to deal with this trouble. For more info, please see my another respo: https://github.com/vince-xunzhe/Convex-Optimizer-in-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gradient Vanishment is not expected in deep learning. Before ReLU, Sigmoid served as the activated function for many Neural Network applications. This might result from the probabilities output from 0 to 1. But the increment is large in central area, small at the sides. It is like the biological Neuron model. But the problems is the gradient of Sigmoid can be vanished with network grows deeper, which cannot train the model efficiently.<br>\n",
    "ReLU solved this problem,as: $$y=max(0,x)$$ and its gradient cannot be vanished after many layers of back-prop.<br>\n",
    "ReLU mimiced the human brain model, and fixed the Gradient Vanishment. From now, we don't need another unsupervised learning layer by layer to initialize the weights.<br>\n",
    "In many real cases, the training speed and accuracy could benefit from replacing Sigmoid with ReLU in hidden layer, but the output layer remained to use the Sigmoid since it represents the probabilistic distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I mentioned Dropout, Adagrad, ReLU... to deal with the multi-layer neural network problems. So what significant benefits we can derive from the deeper network? I can say with the more hidden layers of the neural network, the more abstract the original features can be transformed, and the stronger the model's ability to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I am trying to add a hidden layer to our previously written Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-37a979465ea8>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_units = 784\n",
    "h1_units = 300\n",
    "w1 = tf.Variable(tf.truncated_normal([in_units, h1_units], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([h1_units]))\n",
    "w2 = tf.Variable(tf.zeros([h1_units, 10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, in_units])\n",
    "keep_prob = tf.placeholder(tf.float32) # the ratio of Dropout, the ratio differs in training & prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer formula: $$y=relu(W_1x+b_1)$$\n",
    "the model construction can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "hidden1_drop = tf.nn.dropout(hidden1, keep_prob)\n",
    "y = tf.nn.softmax(tf.matmul(hidden1_drop, w2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the forward computation of the Neural Network were constructed. Then, I will define the loss function & optimizer. (Cross-entropy & Adagrad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(3000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    train_step.run({x: batch_xs, y_: batch_ys, keep_prob: 0.75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP is also known as Fully Connected Network(FCN). It also has limitations, even though we used deeper networks, more hidden nodes, or more epochs, it's hard to get a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
