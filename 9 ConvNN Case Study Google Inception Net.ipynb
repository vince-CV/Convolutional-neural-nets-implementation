{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Case Study: Google Inception Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet has 8 layers, VGGNet has 19 layers, while Inception Net has 22 layers. 1/12 size of parameters compared with that in AlexNet. Why less parameters?\n",
    "* More parameters, more complicated the model, which needs more training data, but high quality data is not that readily available.\n",
    "* More parameters, more expensive computing resources required.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of Inception Net:\n",
    "* Less parameters\n",
    "* Dropped the last FC layer, and repalced with average-pooling layer (90% of parameters cames from FC layer from AlexNet or VGGNet, which also results in over-fitting)  \n",
    "* Inception Module make parameters more efficient (Network in Network idea, NIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIN idea: Smaller network forms a modual, while the larger network can be formed by stacking lots of moduals. Inception Net added branch network, but NIN formed as cascade of conv-layers.<br>\n",
    "In general, to optimize the performance of the conv-layer by increasing output channels, since one filter or kernel only extract one certain feature, but increased channels will also bring large computations and overfits.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPConv from NIN allows to integrate infomation between the channels, which equals to a conv-layer followed by a 1 by 1 conv-layer with ReLU activation.\n",
    "Inception Module was designed more complicated: with 4 branches as figure shows below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Inception Net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 by 1 convolution decreased the time consume compared with 3 by 3, therefore, lower computational resources brings one layer of feature transformation and nonlinearization. In this structure, Inception Module makes networks expand in width and depth, which increases the accuracy but prevented from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain neurons are connected sparsely, and the researchers pointed out that a larger and deeper network should also be sparse, which not only lower the overfit but also lower the computation. Inception Net aims to find out the most optimized Inception Module, whose structure are based on Hebbian princeple (Cells that fire together, wire together). Therefore, highly-correlated nodes connected to form a sparse net.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have more kernels, the outputs were high-correlated at the same place but different channel. 1 by 1 convolution can integrate the features at same spatical place but different channels, and this is also the evidence why 1 by 1 conv was used so frequently in the Inception Nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the highly correlated nodes are connected by different-sized conv-layers (1-1, 3-3, 5-5) in 4 branches of Inception Module, and build an efficient sparse structure based on Hebbian princeple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception Nets has 22 layers, besides the last output layer, it also equiped with auxiliary classifiers which considered a certain layer as classification results and then weighted with 0.3 into final classification results. In this way, it can be considered as model merging, gradients info of back-prop, and extra regularization. It is good for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Inception Nets are a big family:\n",
    "1. Inception V1 (6.67%)\n",
    "2. Inception V2 with Batch Normalization (4.8%)\n",
    "3. Inception V3 with 43 layers (3.5%)\n",
    "4. Inception V4 combined with ResNet from Microsoft (3.08%)\n",
    "\n",
    "   * Batch Normalization: Normalized each mini-batch data in order to make input follow N(1,0) normal distribution, which accelerate the training. Dropout can be eliminated by using BN for network simplification.\n",
    "   * Batch Normalization commonly comes with:\n",
    "       * amplified Learning Rate and Learning Decay\n",
    "       * remove Dropout and lighten L2 normalization\n",
    "       * remove LRN, shuffle the training samples completely\n",
    "       * restrict data augmentation <br>\n",
    "   * By using these hints, V2 trains 14 times faster than that of V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 types of structures of Inception Module has been used in Inception V3, and contrib.slim in Tensorflow can be helpful to implement V3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n",
    "\n",
    "def inception_v3_params(weight_decay=0.00004,\n",
    "                        stddev=0.1,\n",
    "                        batch_norm_var_collection='moving_vars'):\n",
    "    \n",
    "    batch_norm_params = {'decay':0.9997,\n",
    "                         'epsilon':0.001,\n",
    "                         'updates_collections':tf.GraphKeys.UPDATE_OPS,\n",
    "                         'variables_collections':{'beta': None,\n",
    "                                                  'gama': None,\n",
    "                                                  'moving_mean':[batch_norm_var_collection],\n",
    "                                                  'moving_variance':[batch_norm_var_collection] } }\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                           weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "                           activation_fn=tf.nn.relu,\n",
    "                           normalizer_fn=slim.batch_norm,\n",
    "                           normalizer_params=batch_norm_params) as sc:\n",
    "            return sc\n",
    "        \n",
    "\n",
    "def inception_v3_base(inputs, scope=None):\n",
    "    end_points = {}\n",
    "    \n",
    "    with tf.variable_scope(scope, 'InceptionV3',[inputs]):      # Non-Inception Module\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='VALID'):\n",
    "            net = slim.conv2d( inputs,   32, [3,3], stride=2,       scope='Conv_2d_1a_3x3')\n",
    "            net = slim.conv2d(    net,   32, [3,3],                 scope='Conv2d_2a_3*3' )\n",
    "            net = slim.conv2d(    net,   64, [3,3], padding='SAME', scope='Conv2d_2b_3*3' )\n",
    "            net = slim.max_pool2d(net,       [3,3], stride=2,       scope='MaxPool_3a_3*3')\n",
    "            net = slim.conv2d(    net,   80, [1,1],                 scope='Conv2d_3b_1*1' )\n",
    "            net = slim.conv2d(    net,  192, [3,3],                 scope='Conv2d_4a_3*3' )\n",
    "            net = slim.max_pool2d(net,       [3,3], stride=2,       scope='MaxPool_5a_3*3')\n",
    "    \n",
    "    with slim.arg_arg_scope([slim.conv2, slim,max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "        \n",
    "        with tf.variable_scope('Mixed_5b'):  # First Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,48,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,64,[5,5],scope='Conv2d_0b_5*5')                                           \n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,96,[3,3],scope='Conv2d_0b_3*3')\n",
    "                branch_2 = slim.conv2d(branch_2,96,[3,3],scope='Conv2d_0c_3*3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,32,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_5c'):  # Second Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,48,[1,1],scope='Conv2d_0b_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,64,[5,5],scope='Conv_1_0c_5*5')                                           \n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,96,[3,3],scope='Conv2d_0b_3*3')\n",
    "                branch_2 = slim.conv2d(branch_2,96,[3,3],scope='Conv2d_0c_3*3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,64,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_5d'):  # Third Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,48,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,64,[5,5],scope='Conv_1_0b_5*5')                                           \n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,96,[3,3],scope='Conv2d_0b_3*3')\n",
    "                branch_2 = slim.conv2d(branch_2,96,[3,3],scope='Conv2d_0c_3*3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,64,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_6a'):  # First Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,384,[3,3],stride=2, padding='VALID', scope='Conv2d_1a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,64,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,96,[3,3],scope='Conv2d_0b_3*3')\n",
    "                branch_1 = slim.conv2d(branch_1,96,[3,3],stride=2,padding='VALID',scope='Conv2d_1a_1*1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.max_pool2d(net,[3,3],stride=2,padding='VALID',scope='MaxPool_1a_3*3')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_6b'):  # Second Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,128,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,128,[1,7],scope='Conv2d_0b_1*7')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[7,1],scope='Conv2d_0c_7*1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,128,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,128,[7,1],scope='Conv2d_0b_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,128,[1,7],scope='Conv2d_0c_1*7')\n",
    "                branch_2 = slim.conv2d(branch_2,128,[7,1],scope='Conv2d_0d_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[1,7],scope='Conv2d_0e_1*7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_6c'):  # Third Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,160,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,160,[1,7],scope='Conv2d_0b_1*7')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[7,1],scope='Conv2d_0c_7*1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,160,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,160,[7,1],scope='Conv2d_0b_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,160,[1,7],scope='Conv2d_0c_1*7')\n",
    "                branch_2 = slim.conv2d(branch_2,160,[7,1],scope='Conv2d_0d_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[1,7],scope='Conv2d_0e_1*7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_6d'):  # Fourth Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,160,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,160,[1,7],scope='Conv2d_0b_1*7')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[7,1],scope='Conv2d_0c_7*1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,160,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,160,[7,1],scope='Conv2d_0b_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,160,[1,7],scope='Conv2d_0c_1*7')\n",
    "                branch_2 = slim.conv2d(branch_2,160,[7,1],scope='Conv2d_0d_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[1,7],scope='Conv2d_0e_1*7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_6e'):  # Fifth Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[1,7],scope='Conv2d_0b_1*7')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[7,1],scope='Conv2d_0c_7*1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[7,1],scope='Conv2d_0b_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[1,7],scope='Conv2d_0c_1*7')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[7,1],scope='Conv2d_0d_7*1')\n",
    "                branch_2 = slim.conv2d(branch_2,192,[1,7],scope='Conv2d_0e_1*7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "        end_points['Mixed_6e'] = net # as Auxiliary Classifier\n",
    "        \n",
    "        with tf.variable_scope('Mixed_7a'):  # First Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_0 = slim.conv2d(Branch_0,320,[3,3],stride=2,padding='VALID',scope='Conv2d_1a_3*3')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,192,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[1,7],scope='Conv2d_0b_1*7')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[7,1],scope='Conv2d_0c_7*1')\n",
    "                branch_1 = slim.conv2d(branch_1,192,[3,3],stride=2, paddinf='VALID',scope='Conv2d_1a_3*3')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.max_pool2d(net,[3,3],stride=2, padding='VALID',scope='MaxPool_1a_3*3')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_7b'):  # Second Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,320,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,384,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = tf.concat([\n",
    "                    slim.conv2d(branch_1,384,[1,3],scope='Conv2d_0b_1*3'),\n",
    "                    slim.conv2d(branch_1,384,[3,1],scope='Conv2d_0b_3*1')],3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,448,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,384,[3,3],scope='Conv2d_0b_3*3')\n",
    "                branch_2 = tf.concat([\n",
    "                    slim.conv2d(branch_2,384,[1,3],scope='Conv2d_0c_1*3'),\n",
    "                    slim.conv2d(branch_2,384,[3,1],scope='Conv2d_0d_3*1')],3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "        with tf.variable_scope('Mixed_7c'):  # Third Inception Module\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net,320,[1,1],scope='Conv2d_0a_1*1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net,384,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_1 = tf.concat([\n",
    "                    slim.conv2d(branch_1,384,[1,3],scope='Conv2d_0b_1*3'),\n",
    "                    slim.conv2d(branch_1,384,[3,1],scope='Conv2d_0b_3*1')],3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net,448,[1,1],scope='Conv2d_0a_1*1')\n",
    "                branch_2 = slim.conv2d(branch_2,384,[3,3],scope='Conv2d_0b_3*3')\n",
    "                branch_2 = tf.concat([\n",
    "                    slim.conv2d(branch_2,384,[1,3],scope='Conv2d_0c_1*3'),\n",
    "                    slim.conv2d(branch_2,384,[3,1],scope='Conv2d_0d_3*1')],3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3*3')\n",
    "                branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1*1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "        \n",
    "        return net, end_points\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the structure of the Inception V3:\n",
    "    * 5 Conv-layer + 2 Pool-layer\n",
    "    * Inception Module 1\n",
    "    * Inception Module 2\n",
    "    * Inception Module 3\n",
    "Input $229*229$ -> Output $8*8$ <br>\n",
    "Channel: 3(RGB) -> 2048<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enhance the ability of expression:\n",
    "* Branch 1: combining simple featue abstraction;\n",
    "* Branch 2: combining more complex feature abstraction\n",
    "* Branch 3: combining more complex feature abstraction\n",
    "* Branch 4: pooling layer<br>\n",
    "\n",
    "All together, 4 kinds of feature abstraction can selectively retain features, and it can enrich the expression of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Inception V3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3(inputs,\n",
    "                num_classes=1000,\n",
    "                is_training=True,\n",
    "                dropout_keep_prob=0.8,\n",
    "                prediction_fn=slim.softmax,\n",
    "                spatial_squeeze=True,\n",
    "                reuse=None,\n",
    "                scope='InceptionV3'):\n",
    "    \n",
    "    with tf.variable_scope(scope,'InceptionV3',[inputs, num_classes],reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout],is_training=is_training):\n",
    "            net, end_points = inception_v3_base(input, scope=scope)\n",
    "            \n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],stride=1, padding='SAME'):\n",
    "                aux_logits=end_points['Mixed_6e']\n",
    "                \n",
    "                with tf.variable_scope('AugLogits'):\n",
    "                    aux_logits = slim.avg_pool2d(aux_logits,[5,5],stride=3,padding='VALID',scope='AvgPool_1a_5*5')\n",
    "                    aux_logits = slim.conv2d(aux_logits,128,[1,1],scope='Conv2d_1b_1*1')\n",
    "                    aug_logits = slim.conv2d(aux_logits,768,[5,5],weights_initializer=trunc_normal(0.01),\n",
    "                                             padding='VALID',scope='Conv2d_2a_5*5')\n",
    "                    aug_logits = slim.conv2d(aux_logits,num_classes,[1,1],activation_fn=None, normalizer_fn=None,\n",
    "                                             weights_initializer=trunc_normal(0.001),scope='Conv2d_2b_1*1')\n",
    "                    if spatial_squeeze:\n",
    "                        aug_logits = tf.squeeze(aux_logits,[1,2],name='SpatialSqueeze')\n",
    "                    end_points['AuxLogits'] = aug_logits\n",
    "                    \n",
    "                with tf.variable_scope('Logits'):\n",
    "                    net = slim.avg_pool2d(net,[8,8],padding='VALID',scope='AvgPool_1a_8*8')\n",
    "                    net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "                    end_points['PreLogits'] = net\n",
    "                    logits = slim.conv2d(net,num_classes,[1,1],activation_fn=None,normalizer_fn=None,scope='Conv2d_1c_1*1')\n",
    "                    \n",
    "                    if spatial_squeeze:\n",
    "                        logits = tf.squeeze(logits,[1,2],name='SpatialSqueeze')\n",
    "                end_points['Logits'] = logits\n",
    "                end_points['Predictions'] = prediction_fn(logits, scope='Predictions')\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "height, width = 299, 299\n",
    "inputs = tf.random_uniform((batch_size, height, width, 3))\n",
    "with slim.arg_scope(inception_v3_params()):\n",
    "    logits, end_points = inception_v3(inputs, is_training=False)\n",
    "    \n",
    "init = tf.global_varibales_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "num_batches = 100\n",
    "time_tensorflow_run(sess, logits, \"Forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less amount of the computation enable the widespread usage of the Inception nets. In symmary:\n",
    "1. Factorization into small convolutions is efficient, and it lower the parameters, eliminates the overfit, and enhance non-lineaity of the network.\n",
    "2. From input to output, the size would be decreased but the channel would be increased, which converts the spatial information into high-order features.\n",
    "3. Inception Module is efficient and collect different high order features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
